---
title: "Utilizing Bibliometrics for Interdisciplinary Team Building: Hacking Water Management in the Western Balkans"
author: "Maryann Cairns, Daniel J. Hicks"
subtitle: S1 Reproducible Analysis
output: html_document
---

TODO: 
turn plot_net back on

This document provides fully reproducible source code for our entire analysis, including every figure presented in the main paper.  Besides an installation of R and the packages listed below, the following files are required to reproduce the analysis:  

- `combined_metadata.csv`: the primary datafile
- `coauth_net.graphml`: the coauthor network
- `load_data.R`: source code to load the dataset
- `layout.graphml`: the layout for plotting the coauthor network

```{r setup, cache = FALSE, echo = FALSE, message=FALSE, warning=FALSE}
data_folder = '../2016-06-06/'
source('load_data.R')

opts_chunk$set(fig.path = 'figure/', 
				   dev = 'png', dpi = 300, 
				   message = FALSE, warning = FALSE)
options(digits=2)

library(broom)
library(cowplot)
library(dendextend)
library(ggrepel)
library(GGally)
library(knitr)
library(mgcv)
library(RColorBrewer)
library(reshape2)

sessionInfo()
```


# Countries by Components #

Since the sections below restrict the graph to the giant component, here we compare countries to distinct disconnected components of the graph.  

```{r country_component}
country_comp_df = mdf %>% group_by(country, component) %>% summarize(n = n())

ggplot(data = country_comp_df, aes(x = component, y = country, fill = n)) + 
	geom_tile() + 
	scale_fill_gradient(low = 'yellow', high = 'red') +
	scale_x_continuous(breaks = seq(1, max(country_comp_df$component), by = 2)) +
	theme(axis.text.y = element_text(size = 3))

western_balkans = c('Albania', 'Serbia', 'Montenegro', 'Macedonia', 'Kosovo', 'Bosnia and Herzegovina')

ggplot(data = filter(country_comp_df, country %in% western_balkans), 
	   aes(x = component, y = country, fill = n)) + 
	geom_tile() + 
	scale_fill_gradient(low = 'yellow', high = 'red') +
	scale_x_continuous(breaks = seq(1, max(country_comp_df$component), by = 2))
```


# Plot the Network #

This section plots the coauthor network.  

```{r plot_net, dpi=600}
## Load a layout developed in Gephi
layout_file = paste(data_folder, 'layout.yh.san.graphml', sep = '')
graph_layout = read_graph(layout_file, format = 'graphml')
## Copy into the graph
V(graph)$x = V(graph_layout)$x
V(graph)$y = V(graph_layout)$y
## Discard the separate layout
rm(graph_layout)

## ----------
## Restrict to giant component
mdf = mdf %>% filter(component == 1) %>% droplevels
graph = induced_subgraph(graph, components(graph)$membership == 1)
## Identify and remove areas with 0 individuals in the giant component
drop_areas = mdf %>% select(-(id:docs)) %>% summarize_each(funs(sum)) %>%
	melt() %>% filter(value == 0) %>% .[['variable']] %>% as.character
mdf = mdf %>% select(-one_of(drop_areas))
## ----------

## Graph statistics
summary(graph)

## Cluster using edge betweenness
communities = cluster_edge_betweenness(graph)
## Use betweenness for size
V(graph)$btwn = betweenness(graph, normalized = TRUE)

## Plot the graph
## Output to tikzDevice:
##  NB use vertex.label.cex = .4
#tikz(paste(data_folder, 'graph.tex', sep = ''),standAlone = TRUE)
## Output to high-res PNG
#png(filename = paste(data_folder, 'graph.png', sep = ''), width = 4000, height = 4000)
# plot(graph,
# 	 vertex.size = 2 + 30 * V(graph)$btwn,
# 	 #vertex.color = communities$membership,
# 	 vertex.color = as.factor(V(graph)$country %in% western_balkans),
# 	 vertex.frame.color = NA,
# 	 #vertex.label = NA,
# 	 vertex.label = V(graph)$country,
# 	 #vertex.label = V(graph)$surname,
# 	 vertex.label.cex = (.1 + 1 * V(graph)$btwn),
# 	 vertex.label.color = 'black',
# 	 edge.width = .5, edge.curved = FALSE)
#dev.off()
```


# Individual Researcher Statistics #

This section generates tables and plots for various individual-level statistics. 

```{r individual_stats}
## Individual researcher statistics
## How many individuals?  
nrow(mdf)

## Top 10 researchers by betweenness centrality
mdf %>% select(surname, given, affiliation, country, deg, docs, btwn) %>% arrange(desc(btwn)) %>% head(10) %>% kable

## Degree and betweenness centrality
deg_btwn_plot = ggplot(data = mdf, aes(x = btwn, y = deg)) +
	geom_point() + stat_smooth(method = 'lm') +
	scale_y_log10(name = 'degree') + 
	scale_x_log10(name = 'centrality')
deg_btwn_plot
summary(lm(data = filter(mdf, btwn > 0), log10(btwn) ~ log10(deg)))

## Number of documents and betweenness centrality
docs_btwn_plot = ggplot(data = mdf, aes(x = btwn, y = docs)) +
	geom_point() + stat_smooth(method = 'lm') +
	scale_y_log10(name = 'publications') +
	scale_x_log10(name = 'centrality')
docs_btwn_plot
summary(lm(data = filter(mdf, btwn > 0, docs > 0), log10(btwn) ~ log10(docs)))

plot_grid(deg_btwn_plot, docs_btwn_plot, ncol = 1, labels = 'AUTO') 
```



# Country-Level Statistics #

This section calculates various country-level statistics, then generates tables and plots for them.  Because the country-wide distributions of degree, betweenness centrality, and number of documents are highly skewed, we generally use the median rather than the mean.  

```{r country_stats}
## Country-level stats
country_df = mdf %>% group_by(country) %>% 
	mutate(btwn_log = log10(btwn)) %>%
	summarize_each_(vars = c('deg', 'btwn', 'btwn_log', 'docs'), 
					funs(sum, mean, max, median, sd, IQR))
country_df[country_df == -Inf|country_df == Inf] = NA
country_df = mdf %>% group_by(country) %>% summarize(n_authors = n()) %>% left_join(country_df)

## HDI data
## http://hdr.undp.org/en/data#
hdi_df = read.csv('Human Development Index (HDI).csv', skip = 1)
hdi_df$hdi = hdi_df %>% select(X1980:X2010) %>% rowMeans(na.rm = TRUE)
hdi_df = hdi_df %>% transmute(country = gsub('^\\s', '', Country), 
					 hdi = hdi)
country_df = left_join(country_df, hdi_df)

## GDP data
## http://databank.worldbank.org/data/reports.aspx?source=2&series=NY.GDP.MKTP.CD&country=
gdp_df = read.csv('Data_Extract_From_World_Development_Indicators/e21b37e3-d24e-4a17-bc1c-0a13efb9929c_Data.csv')

gdp_df =  select(gdp_df, Country.Name, X1991..YR1991.:X2015..YR2015.)
names(gdp_df) = c('country', paste('yr', 1991:2015, sep = ''))
gdp_countries = gdp_df$country
## Manually fix a few country names
gdp_countries = gsub('Iran, Islamic Rep.', 'Iran', gdp_countries)
gdp_countries = gsub('Macedonia, FYR', 'Macedonia', gdp_countries)
gdp_countries = gsub('Slovak Republic', 'Slovakia', gdp_countries)
gdp_countries = gsub('Korea, Rep.', 'South Korea', gdp_countries)

parse = function(x) {as.numeric(as.character(x))}
gdp = gdp_df %>% select(yr1991:yr2015) %>% mutate_each(funs(parse)) %>%
	rowMeans(na.rm = TRUE)
country_df = data.frame(country = gdp_countries, gdp = gdp) %>% 
	right_join(country_df)
```
```{r country_stats_plots}
## How many countries?
nrow(country_df)

## Pairs plot
geom_scatter_smooth = function(data, mapping, method = 'lm', ...) {
	p = ggplot(data = data, mapping = mapping) +
		geom_point() +
		geom_smooth(method = method, ...)
	p
}
ggpairs(country_df, columns = c('n_authors', 'btwn_log_median',
								'hdi', 'gdp'), 
		lower = list(continuous = wrap(geom_scatter_smooth, method = 'lm'))) +
	theme(axis.text = element_text(size = 4), 
		  axis.title = element_text(size = 6))

## Countries, by descending count of authors
country_df %>% select(country, n_authors, btwn_median) %>% arrange(desc(n_authors)) %>% kable
## Plot of countries, by descending count of authors
ggplot(data = country_df, aes(x = reorder(country, n_authors), y = n_authors, fill = country)) + 
	geom_bar(stat = 'identity') +
	xlab('country') + ylab('no. authors') +
	guides(fill = FALSE) +
	coord_flip() +
	theme(axis.text.y = element_text(size = 5))


## Countries, by descending median number of papers
country_df %>% select(country, docs_median) %>% arrange(desc(docs_median)) %>%
	kable(digits = 2)
## Plot
ggplot(data = country_df, aes(x = reorder(country, docs_median), 
							  y = docs_median, fill = country)) + 
	geom_bar(stat = 'identity') +
	xlab('country') + ylab('median no. papers') +
	guides(fill = FALSE) +
	coord_flip() +
	theme(axis.text.y = element_text(size = 5))


## Countries, by descending sum centrality
country_df %>% select(country, n_authors, btwn_sum, btwn_median) %>% arrange(desc(btwn_sum)) %>% 
	filter(btwn_sum > 0) %>%
	kable(digits = 2)
ggplot(data = filter(country_df, btwn_sum > max(btwn_sum)/100), 
	   aes(x = reorder(country, btwn_sum), y = btwn_sum, 
	   	fill = reorder(country, desc(btwn_sum)))) +
	geom_bar(stat = 'identity') +
	xlab('') + 
	ylab('aggregate centrality') +
	guides(fill = FALSE) +
	#scale_fill_brewer(palette = 'Set1') +
	scale_fill_manual(values = rep(brewer.pal(9, 'Set1'), 2)) +
	coord_flip()

## Countries, by descending median centrality
country_df %>% select(country, btwn_median) %>% 
	filter(btwn_median > 0) %>%
	arrange(desc(btwn_median)) %>% kable
# ggplot(data = filter(country_df, btwn_median > max(btwn_median)/100), 
# 	   aes(x = reorder(country, btwn_median), y = btwn_median)) +
# 	geom_bar(stat = 'identity') +
# 	xlab('country') + 
# 	ylab('median centrality') +
# 	coord_flip()

btwn_plot_countries = country_df %>% filter(btwn_median > 0) %>% .[['country']]
## Barplot of medians, with jittered individual values
ggplot(data = filter(country_df, country %in% btwn_plot_countries),
					 aes(x = reorder(country, btwn_median), 
							  y = btwn_median, color = country, fill = country)) +
	geom_bar(stat = 'identity', width = .05) +
	geom_point(data = filter(mdf, country %in% btwn_plot_countries), 
			   aes(x = country, y = btwn), position = 'jitter') +
	guides(color = FALSE, fill = FALSE) +
	ylab('centrality (median)') + #scale_y_log10() +
	xlab('country') +
	coord_flip()

## Individual values, country medians marked with X
##  NB scale_y_log10 applies the log before calculating the medians, 
##    producing errors if we try to use stat_summary, so we 
##    calculate medians in the data assignment
country_stats_plot_1 = ggplot(data = {mdf %>% 
				filter(country %in% btwn_plot_countries) %>%
				group_by(country) %>% mutate(median_btwn = median(btwn))}, 
	   aes(x = reorder(country, btwn, FUN = median), y = btwn)) +
	geom_point(aes(color = country), 
	   position = 'jitter') + 
	#stat_summary(fun.y = 'median', geom = 'line') +
	geom_point(aes(y = median_btwn), shape = 'X', size = 3, color = 'black') +
	guides(color = FALSE, fill = FALSE) +
	ylab('centrality') + scale_y_log10() +
	xlab('') +
	coord_flip() + 
	theme(axis.text.y = element_text(size = 6))
country_stats_plot_1
			

## Relationship between number of authors and median centrality
country_stats_plot_2 = ggplot(data = filter(country_df, btwn_sum > 0), 
	   aes(btwn_median, n_authors)) + 
	geom_point() +
	geom_smooth(method = 'lm') +
	guides(color = FALSE) + scale_x_log10() + scale_y_log10() +
	ylab('no. authors') + xlab('centrality (median)')
country_stats_plot_2 +
	geom_label_repel(aes(label = country), size = 1.5)
summary(lm(data = filter(country_df, btwn_sum > 0, btwn_median > 0), 
		   log(btwn_median) ~ log(n_authors)))

## Relationship between median and IQR centrality
country_stats_plot_3 = ggplot(data = filter(country_df, btwn_sum > 0), 
	   aes(btwn_median, btwn_IQR)) +
	geom_point() +
	geom_smooth(method = 'lm') +
	guides(color = FALSE) + scale_x_log10() + scale_y_log10() +
	ylab('centrality (IQR)') + xlab('centrality (median)')
country_stats_plot_3 +
	geom_label_repel(aes(label = country), size = 1.5)
summary(lm(data = filter(country_df, btwn_sum > 0, btwn_median > 0, btwn_IQR > 0), 
		   log(btwn_median) ~ log(btwn_IQR)))
```
```{r country_stats_comb_plot, fig.height = 7, fig.width = 7}
## Combine the last three plots
plot_grid(country_stats_plot_1, 
		  plot_grid(country_stats_plot_2, country_stats_plot_3, 
		  		  labels = c('B', 'C'), align = 'hv'),
		  labels = c('A', ''), ncol = 1, rel_heights = c(1, 1))
```


# Country-Level Network #

This section plots the country-level network.  

```{r country_net, dpi = 600, fig.height = 9, fig.width = 9}
## Extract the edgelist from the coauthor net, 
##  convert the labels into countries, 
##  and turn into a graph
country_edgelist = as_edgelist(graph) %>%
	apply(., MARGIN = 2, 
		  FUN = function (x) get.vertex.attribute(graph, 'country', index = x))
country_edgelist[is.na(country_edgelist)] = ''
country_edgelist = as.data.frame(country_edgelist)
country_net = graph_from_data_frame(country_edgelist, directed = FALSE)

E(country_net)$weight = 1
country_net = simplify(country_net, edge.attr.comb=list(weight="sum"), 
					   remove.loops = FALSE)
#is_simple(country_net)

# 
# ## Simplify puts the weights into edges, but we want loop weights in vertices
# ## So we calculate them manually
# n_loops = country_edgelist %>%
# 	mutate(V1 = as.character(V1), V2 = as.character(V2)) %>%
# 	filter(V1 == V2) %>% group_by(V1) %>% summarize(n = n())
# ## Interpolate vertices without loops
# n_loops = left_join(data.frame(V1 = V(country_net)$name), n_loops)
# ## Match to the order of country_net
# n_loops = n_loops[match(V(country_net)$name, n_loops$V1),]
# ## Replace NAs with 0s
# n_loops[is.na(n_loops)] = 0
# ## Write into the graph
# V(country_net)$n_loops = n_loops$n
# #as_data_frame(country_net, what = 'vertices')

## Divide into clusters and write into graph
communities = cluster_walktrap(country_net)
plot(as.dendrogram(communities), horiz = TRUE)
## Divide into clusters using a manual cut, rather than an optimizing one
#communities(communities)
V(country_net)$cluster = #membership(communities)
	cut_at(communities, steps = 40)

## Uncomment to save
#write_graph(country_net, 'country_net.graphml', format = 'graphml')

## Arrange in a circle
layout_country = layout_in_circle(country_net, 
								  #order = order(V(country_net)$name))
								  #order = order(membership(communities))
								  order = order(V(country_net)$cluster))
## Write layout into graph
V(country_net)$x = layout_country[,1]
V(country_net)$y = layout_country[,2]

## Uncomment the next line and last line to save to file
#png(filename = 'country_net.png', width = 2000, height = 2000)
plot(country_net, #layout = layout_country, 
	 vertex.size = 4 * log(1 + sapply(V(country_net)$name, 
	 								 function (x) sum(mdf$country == x, 
	 								 				 na.rm = TRUE))),
	 vertex.label.dist = .25,
	 vertex.label.degree = suppressWarnings(
		 c(1,-1) * atan2(V(country_net)$y, V(country_net)$x)),
	 #vertex.label.color = communities$membership,
	 vertex.label.color = 'black',
	 vertex.label.cex = .6,
	 #vertex.color = 'grey60', 
	 vertex.color = V(country_net)$cluster,
	 vertex.frame.color = ifelse(V(country_net)$name %in% western_balkans,
	 					  'black', NA),
	 vertex.shape = ifelse(V(country_net)$name %in% western_balkans,
	 					  'csquare', 'circle'),
	 edge.curved = TRUE,
	 edge.color = 'grey60',
	 edge.width = .025 * E(country_net)$weight, 
	 edge.loop.angle = -atan2(head_of(country_net, E(country_net))$y,
	 						  head_of(country_net, E(country_net))$x),
	 palette = brewer.pal(7, 'Set1'), 
	 margin = 1)
#box()
#dev.off()
```



# Areas or Keywords #

This section analyzes the areas or keywords information.  We first identify the 15, 30, and 45 most common areas in the dataset, and determine what percentage of researchers are covered by these sets of common areas.  

```{r areas_coverage}
## Extract the list of areas
areas = mdf %>% select(-(id:docs)) %>% names

## Number of areas
length(areas)

## Coverage of the top 15, 30, and 45 areas
areas_15 = mdf %>% select(one_of(areas)) %>% summarize_each('sum') %>% 
	sort(decreasing = TRUE) %>% names %>% head(n=15)

areas_30 = mdf %>% select(one_of(areas)) %>% summarize_each('sum') %>%
	sort(decreasing = TRUE) %>% names %>% head(n=30)

areas_45 = mdf %>% select(one_of(areas)) %>% summarize_each('sum') %>%
	sort(decreasing = TRUE) %>% names %>% head(n=45)

areas_coverage = function (areas) {
	mdf %>% select(one_of(areas)) %>% rowSums() %>% 
		magrittr::is_greater_than(0) %>%
		sum %>% magrittr::divide_by(nrow(mdf))
}

## Function to collapse a list of areas and replace dots with spaces
areas_to_string = function (areas) {
	areas %>% paste(collapse = '; ') %>% 
		gsub('\\.\\.', '\\.', .) %>% gsub('\\.', ' ', .)
}

areas_list = list(areas_15 = areas_15, areas_30 = areas_30, areas_45 = areas_45)
areas_coverage_df = data.frame(
						areas = sapply(areas_list, areas_to_string),
						coverage = 100 * sapply(areas_list, areas_coverage)
)

areas_coverage_df %>% kable(digits = 0)
```

```{r areas_wb}
## Top 15 areas in the Western Balkans
areas_15_wb = mdf %>% filter(country %in% western_balkans) %>%
	select(one_of(areas)) %>% summarize_each('sum') %>% 
	sort(decreasing = TRUE) %>% names %>% head(n=15)
areas_15_wb
```

We can also use cluster analysis, treating similar areas of interest or similar individual researchers as a measure of "distance" between researchers and areas, respectively.  For individuals, we can calculate distances using either the top-15 or top-45 areas.  The following plots show the cumulative distribution of these distances across coauthor pairs.  Both plots can be closely approximated by uniform distributions.  

```{r areas_distances}
edge_dist_45 = apply(as_edgelist(graph), 1, 
					 function (x) (dist(mdf[x, areas_45], method = 'binary')[1]))
edge_dist_15 = apply(as_edgelist(graph), 1, 
					 function (x) (dist(mdf[x, areas_15], method = 'binary')[1]))

edge_dist_45_plot = ggplot(data.frame(x = edge_dist_45), aes(x)) + 
	geom_segment(aes(x = 0.25, y = 0, xend = 1, yend = 1), color = 'blue') +
	stat_ecdf() + 
	coord_flip() +
	xlab('45-area distance') + 
	scale_y_continuous(name = 'cumulative percentile', labels = scales::percent)
	
edge_dist_15_plot = ggplot(data.frame(x = edge_dist_15), aes(x)) + 
	geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), color = 'blue') +
	stat_ecdf() + 
	coord_flip() +
	labs(x = '15-area distance', y = 'cumulative quantile') +
	scale_y_continuous(name = 'cumulative percentile', labels = scales::percent)

plot_grid(edge_dist_15_plot, edge_dist_45_plot, labels = 'AUTO')
```	

Specifically, using the 15-area distance, collaborations between researchers with substantially different interests (e.g., near .75, `r target = .75; ecdf(edge_dist_15)(target + .01) - ecdf(edge_dist_15)(target - .01) * 100`%) are only slightly more frequent than collaborations between researchers with fairly similar interests (e.g., near .25, `r target = .25; ecdf(edge_dist_15)(target + .01) - ecdf(edge_dist_15)(target - .01)`).  

For areas, we identify clusters among the top 45 areas, based on similarities in the distribution of individual researchers.  

```{r areas_cluster}
## Cluster analysis
areas_data = mdf %>% select(one_of(areas_45))
areas_dendro = dist(t(areas_data*1), method = 'binary') %>%
	hclust %>% as.dendrogram %>%
	color_branches(h = .8) %>%
	color_labels(h = .8) %>%
	set('labels_cex', .5)
#ggplot(as.ggdend(areas_dendro), horiz = TRUE) + scale_y_reverse(limits = c(1, -.5))


areas_data = mdf %>% select(one_of('id', areas_45)) %>% 
				melt(id.vars = 'id', variable.name = 'area') %>%
				filter(value) %>% 
				mutate(cluster = cutree(areas_dendro, h = .8)[area], 
					   area = reorder(area, cluster)) %>%
				mutate(area = gsub('\\.', ' ', area))

ggplot(data = areas_data, 
	   aes(id, reorder(area, cluster), fill = as.factor(value * cluster))) + 
	geom_tile() +
	scale_x_discrete(breaks = NULL, name = '') + 
	ylab('') + 
	scale_fill_manual(values = rep(brewer.pal(5, 'Set1'), 4), guide = FALSE) +
	theme(axis.text.y = element_text(size = 8))
```


We can construct a network of individuals and the top 45 areas, although the results are difficult to interpret.  

```{r areas_network, eval=FALSE}
areas_network = mdf %>% select(one_of('id', areas_30)) %>% 
	melt(id.vars = 'id', variable.name = 'area') %>%
	filter(value) %>% 
	graph_from_data_frame(directed = FALSE)

bipartite_mapping(areas_network)$res

ids = mdf$id
V(areas_network)$type = V(areas_network)$name %in% ids

plot(areas_network, 
	 #layout = layout_with_dh(areas_network), # too slow! 
	 ## The next two create trees, but edges and node labels overlap
	 #layout = layout_with_sugiyama(areas_network)$layout,
	 #layout = layout_as_tree(areas_network),
	 ## The next three cluster the areas nodes in the middle
	 #layout = layout_with_fr(areas_network),
	 #layout = layout_with_graphopt(areas_network),
	 layout = layout_with_kk(areas_network),
	 vertex.label = ifelse(V(areas_network)$type,
	 					  '',
	 					  V(areas_network)$name),
	 vertex.label.cex = .5,
	 vertex.label.color = 'black',
	 vertex.size = 10 * (1 - V(areas_network)$type), 
	 vertex.color = V(areas_network)$type,
	 edge.width = .1)
```

Next we use heatmaps to compare countries and areas.  

```{r countries_areas_heatmaps}
## Country x areas
country_areas = mdf %>% group_by(country) %>% select(one_of(areas)) %>% 
	summarize_each('sum') %>% 
	melt(id.vars = 'country') %>% 
	filter(value != 0)

## Heatmap: all countries and areas
ggplot(data = country_areas, 
	   aes(country, variable, fill = value)) + 
	geom_tile() + 
	scale_fill_gradient(low = 'yellow', high = 'red') +
	theme(axis.text.x = element_text(hjust = 1, angle = 45), 
		  axis.text.y = element_text(size = 3))

## Heatmap: countries and top 45 areas
ggplot(data = filter(country_areas, variable %in% areas_45), 
	   aes(country, variable, fill = value)) +
	geom_tile() +
	scale_fill_gradient(low = 'yellow', high = 'red') +
	theme(axis.text.x = element_text(hjust = 1, angle = 45, size = 4), 
		  axis.text.y = element_text(size = 4))

## Heatmap:  countries and areas with more than a certain number of individuals
# ggplot(data = {country_areas %>% filter(value > 10)}, 
# 	   aes(country, variable, fill = value)) +
# 	geom_tile() + 
# 	scale_fill_gradient(low = 'yellow', high = 'red')+
# 	theme(axis.text.x = element_text(hjust = 1, angle = 45), 
# 		  axis.text.y = element_text(size = 2))

## Heatmap:  Western Balkans and top 45 areas
ggplot(data = {country_areas %>% filter(country %in% western_balkans, 
										variable %in% areas_45) %>%
				mutate(variable = gsub('\\.', ' ', variable))}, 
	   aes(country, reorder(variable, desc(variable)), fill = value)) +
	geom_tile() + 
	scale_fill_gradient(low = 'yellow', high = 'red', name = 'no.\nauthors') +
	xlab('') + ylab('') +
	theme(axis.text.x = element_text(hjust = 1, angle = 20), 
		  axis.text.y = element_text(size = 6))
```

Finally, we can calculate a "score" for each country, based on the number of the top 45 areas in which it has at least one researcher.  This variable has a strong association with the number of authors, and is fairly well-explained by the combination of the number of authors and GDP.  

```{r areas_score}
## Calculate an areas score and add it to country_df
country_df = country_areas %>% filter(variable %in% areas_45) %>%
	group_by(country) %>% summarize(areas.45.score = sum(value > 0)) %>%
	right_join(country_df)

ggpairs(country_df, columns = c('n_authors', 'btwn_log_median',
								'areas.45.score',
								'hdi', 'gdp'), 
		lower = list(continuous = wrap(geom_scatter_smooth, method = 'lm'))) +
	theme(axis.text = element_text(size = 4), 
		  axis.title = element_text(size = 6))
areas_auth_gdp_fit = lm(data = country_df, areas.45.score ~ n_authors + I(gdp/1000))
kable({tidy(areas_auth_gdp_fit) %>% 
		mutate(ci.low = estimate - qnorm(.975)*std.error, 
			   ci.high = estimate + qnorm(.975)*std.error)}, 
	  caption = paste('Linear regression of top 45 areas score against GDP and number of authors. ', 
	  	'(R^2 = ', glance(areas_auth_gdp_fit)$r.squared, 
	  	')', sep = ''))

# ## Areas score and no. authors
# ggplot(data = {country_areas %>% filter(variable %in% areas_45) %>%
# 		group_by(country) %>% summarize(areas.45.score = sum(value > 0)) %>%
# 		right_join(country_df)}, 
# 	aes(n_authors, areas.45.score)) + 
# 	geom_point() + 
# 	geom_smooth(method = 'lm') +
# 	scale_x_log10()
# 
# ## Areas score and median betweenness
# ggplot(data = {country_areas %>% filter(variable %in% areas_45) %>%
# 		group_by(country) %>% summarize(areas.45.score = sum(value > 0)) %>%
# 		right_join(country_df)}, 
# 	aes(areas.45.score, btwn_median)) + 
# 	geom_point() + 
# 	scale_y_log10()
# 
# ## Areas score and HDI
# ggplot(data = {country_areas %>% filter(variable %in% areas_45) %>%
# 			group_by(country) %>% summarize(areas.45.score = sum(value > 0)) %>%
# 			right_join(country_df)}, 
# 		aes(hdi, areas.45.score)) + 
# 	geom_point() + geom_smooth(method = 'lm') + 
# 	scale_y_log10()
```


# Missed Connections #

*Missed connections* are pairs of individuals who have similar research interests (similar areas), but are distant in the coauthor network.  This distance may be due to collaborations that Scopus is not aware of, or it may be due to actual social distance.  We operationalize missed connections using a nonlinear predictor (a Generalized Additive Model) that regresses actual network distance against area similarity.  A pair is a missed connection when the actual network distance is more than 3.5 times the distance predicted based on area similarity.  

```{r missed_connections, eval=TRUE}
## Calculate distance based on areas
areas = mdf %>% select(-(id:docs)) %>% names
areas_based_dist = dist({mdf %>% select(one_of(areas_45))}, method = 'binary') %>%
	as.matrix()
dimnames(areas_based_dist) = list(mdf$sid, mdf$sid)
## Melt into dataframe
dist_df = melt(areas_based_dist, value.name = 'areas.distance', 
						 as.is = TRUE)

## Calculate distance based on graph
paths_dist = shortest.paths(graph)
dimnames(paths_dist) = list(V(graph)$sid, V(graph)$sid)
## Melt into dataframe and join with the areas-based distance
dist_df = melt(paths_dist, value.name = 'path.distance', as.is = TRUE) %>% 
	full_join(dist_df)
## Fit a GAM, and use its predictions to identify missed connections
dist_df$path.dist.pred = 
	gam(path.distance ~ s(areas.distance, bs = 'cs'), 
		data = dist_df)$fitted.values
dist_df = dist_df %>% mutate(missed.connection = path.distance > 2.5 * path.dist.pred)
## Clean up
rm(areas_based_dist, paths_dist)

## How many missed connections?
dist_df %>% filter(Var1 < Var2, missed.connection) %>% nrow()

## Plotting areas vs. path distance, the smooth, and missed connections
ggplot(data = filter(dist_df, Var1 < Var2), 
	   aes(areas.distance, path.distance)) + 
	geom_point(aes(color = missed.connection, alpha = missed.connection), 
			   position = 'jitter') + 
	scale_color_brewer(palette = 'Set1', guide = FALSE) +
	scale_alpha_discrete(range = c(.01, 1), guide = FALSE) +
	geom_line(aes(areas.distance, path.dist.pred), alpha = 1, size = 2) +
	xlab('areas-based distance') + ylab('network-based distance')

## Extract the missed connections
missed_df = dist_df %>% filter(missed.connection)

who_is = function (sid) {
	sid = as.character(sid)
	mdf[which(mdf$sid == sid), c('sid', 'given', 'surname', 'country')]
}

var1_df = lapply(missed_df[['Var1']], who_is) %>% do.call(rbind, .)
names(var1_df) = c('Var1', 'given.1', 'surname.1', 'country.1')

var2_df = lapply(missed_df[['Var2']], who_is) %>% do.call(rbind, .)
names(var2_df) = c('Var2', 'given.2', 'surname.2', 'country.2')

missed_df = left_join(missed_df, cbind(var1_df, var2_df))
rm(who_is, var1_df, var2_df)

ggplot(data = {missed_df %>% group_by(country.1, country.2) %>% 
				summarize(n = n()) %>% na.omit()},
	   aes(country.1, reorder(country.2, desc(country.2)), fill = n)) +
	geom_tile() + scale_fill_gradient(low = 'yellow', high = 'red') + 
	stat_function(fun = function(x) (34 - x)) +
	xlab('') + ylab('') +
	theme(axis.text.x = element_text(hjust = 1, angle = 30, size = 6), 
		  axis.text.y = element_text(size = 6))
```
