# -*- coding: utf-8 -*-
'''
This module defines several functions used to scrape article metadata, 
primarily from Scopus but also from PubMed.  

Note that these functions need a Scopus API key defined in `api_key.py`.
A new Scopus API key can be generated by registering for free [on the Scopus API page](http://dev.elsevier.com/index.html). 
'''

#from collections import OrderedDict
import requests
import json
#from math import ceil
import time # Used to pause after receiving a timeout error
import xmltodict

from api_key import MY_API_KEY

class ParseError(Exception):
	pass

def _parse_coauth_data(sid, response_raw):
	'''
	Given the `requests.Response`, parse the XML metadata.
	:param sid: The Scopus ID for the queried author
	:param response_raw: XML metadata, retrieved from Scopus using requests.get
	:return: List of pairs of SIDs, one for each identified coauthor pair
	'''
	# Convert the xml response to a dict to make it easier to parse
	response = json.loads(response_raw)
	#print 'parsed to dict'
	# This branch catches error codes in the response
	if 'service-error' in response:
		# If the resource isn't found, we'll just get a bunch of key errors;
		#  return an empty set of metadata
		if response['service-error']['status']['statusCode'] == 'INVALID_INPUT':
			print('\t\tResource not found error')
			return([])
		# This seems to be what happens if Scopus doesn't have any coauthors on record
		elif response['service-error']['status']['statusText'] == 'There was a search failure':
			print('\t\tNo coauthors found')
			return([])	
		# If something else is going on, raise an exception
		else:
			print(response)
			raise ParseError('Service error in query response')
	# The locations of these metadata are given in the Scopus XML documentation
	# http://ebrp.elsevier.com/pdf/Scopus_Custom_Data_Documentation_v4.pdf
	# If a field is missing, the dict raises a KeyError or TypeError 
	#  (`'NoneType' object is not subscriptable`), and so we use an empty string 
	#  instead
	entries = response['search-results']['entry']
	pairs = [(sid, entry['dc:identifier'].split(':')[1]) for entry in entries]
	return(pairs)

def _parse_auth_data(response_raw):
	'''
	Parse author data from within `get_auth_data_by_sid`
	'''
	# Convert the xml response to a dict to make it easier to parse
	#response = json.loads(response_raw)
	response = xmltodict.parse(response_raw)
	# This branch catches error codes in the response
	if 'service-error' in response:
		if response['service-error']['status']['statusCode'] == 'INVALID_INPUT':
			print('\t\tResource not found error')
			return([])
		# If something else is going on, raise an exception
		else:
			print(response)
			raise ParseError('Service error in query response')
	response = response['author-retrieval-response']
	sid = response['coredata']['dc:identifier'].split(':')[1]
	try:
		docs = int(response['coredata']['document-count'])
	except TypeError:
		docs = 0
	try:
		areas = response['subject-areas']['subject-area']
	except TypeError:
		areas = []
	if type(areas) == 'list':
		areas = [area['#text'] for area in areas]
	
	response = response['author-profile']
	name = response['preferred-name']
	name = {'surname': name['surname'], 'given': name['given-name']}
	
	# Parsing the affiliation fields
	try:	
		affiliation_dict = response['affiliation-current']['affiliation']['ip-doc']
	except KeyError:
		affiliation = ''
		country = ''
	else:
		# University/institution name
		if 'parent-preferred-name' in affiliation_dict:
			affiliation = affiliation_dict['parent-preferred-name']
		elif 'preferred-name' in affiliation_dict:
			affiliation = affiliation_dict['preferred-name']
		else:
			affiliation = ''
		# Country
		if 'address' not in affiliation_dict:
			country = ''
		elif affiliation_dict['address'] is None:
			country = ''
		else:
			if 'country' in affiliation_dict['address']:
				country = affiliation_dict['address']['country']
			elif '@country' in affiliation_dict['address']:
				country = affiliation_dict['address']['@country']
			else:
				country = ''

	meta = {'name': name, 
			'sid': sid, 'docs': docs, 
			'areas': areas, 'affiliation': affiliation, 
			'country': country}
	return meta
			
def _get_query(query):
	'''
	Get an HTTP query, with some wrapping to handle timeouts.
	:param query: The HTTP query string
	:return: The requests.get response
	'''
	# Timeout for HTTP requests
	TIMEOUT = 60
	# Delay, in seconds, after receiving a timeout
	DELAY = 3*60
	# Maximum number of attempts to make before throwing an error
	MAX_ATTEMPTS = 5
	
	attempts = 0
	while (attempts < MAX_ATTEMPTS):
		attempts += 1
		try:
			response_raw = requests.get(query, 
							#headers = {'X-ELS-APIKey': MY_API_KEY}, 
							timeout = TIMEOUT)
			return(response_raw.text)
		except requests.exceptions.Timeout:
			print('Request timed out.  Cooldown for ' + str(DELAY) + ' seconds.')
			time.sleep(DELAY)
	else:
		raise requests.exceptions.Timeout('Maximum number of requests')


def get_coauths_by_sid(sid):
	'''
	Use Scopus to identify coauthors.  Returns a list of pairs of SIDs.  
	'''
	# Build the http query, and send it using `_get_query`
	base_query = 'http://api.elsevier.com/content/search/author?'
	query = base_query + 'co-author=' + sid + \
		'&count=200' + '&apiKey=' + MY_API_KEY
	print('\t' + query)
	response_raw = _get_query(query)
	meta = _parse_coauth_data(sid, response_raw)
	return meta
	
def get_auth_data_by_sid(sid):
	'''
	Use Scopus to retrieve author data.  Returns a list containing the dict 
	of author data.  
	'''
	base_query = 'http://api.elsevier.com/content/author/author_id/'
	query = base_query + sid + '?' + 'apiKey=' + MY_API_KEY
	print('\t' + query)
	response_raw = _get_query(query)
	meta = _parse_auth_data(response_raw)
	return [meta]


if __name__ == '__main__':
	author_list = ['25225396500']
	#author_list = ['1111111111']
	for author in author_list:
		metadata = get_auth_data_by_sid(author)
		#print(json.dumps(metadata, indent = 4, ensure_ascii = False))
		print(metadata)